{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pliki xml, z ktorych czytamy musza miec tag root"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "PATH = 'train_2015_10_22.utf-8/STSint.input.headlines.xml'\n",
    "PATH_PROCESSED = 'train_2015_10_22.utf-8/processed_xml.xml'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "with open(PATH, 'rb') as reader:\n",
    "    with open(PATH_PROCESSED, 'wb') as writer:\n",
    "        text = reader.read()\n",
    "        text = text.replace(b'<==>', b'---')\n",
    "        text = text.replace(b'&', b'und')\n",
    "        writer.write(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "def clear_sentence(text):\n",
    "    text = text.replace('//', '')\n",
    "    text = text.strip()\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "def clear_alignment(text):\n",
    "    new_text= []\n",
    "    for chunk in text.split('\\n'):\n",
    "        chunks = chunk.split('//')\n",
    "\n",
    "        alignment = chunks[0]\n",
    "        class_1 = chunks[1].strip()\n",
    "        class_2 = chunks[2].strip()\n",
    "\n",
    "        chunks_text = chunks[3].split('---')\n",
    "        chunks_text[0] = chunks_text[0].strip()\n",
    "        chunks_text[1] = chunks_text[1].strip()\n",
    "\n",
    "        alignments = alignment.split('---')\n",
    "        alignment_source = alignments[0].strip()\n",
    "        alignment_translation = alignments[1].strip()\n",
    "\n",
    "        alignment_source = tuple(map(int, alignment_source.split(' ')))\n",
    "        alignment_translation = tuple(map(int, alignment_translation.split(' ')))\n",
    "\n",
    "        row = [class_1, class_2, alignment_source, alignment_translation, chunks_text]\n",
    "        new_text.append(row)\n",
    "    return new_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "def source_to_words(text):\n",
    "    text.replace('\\n', '')\n",
    "    chunks = text.split(':')\n",
    "    result = []\n",
    "    for chunk in chunks:\n",
    "        chunk = chunk.strip()\n",
    "        chunk = chunk[1:]\n",
    "        chunk = chunk.strip()\n",
    "        result.append(chunk)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "def df_from_alignment(series):\n",
    "    new_df = pd.DataFrame(columns=['class_1', 'class_2', 'chunks_source', 'chunks_translation', 'text_source', 'text_translation'])\n",
    "    for data in series:\n",
    "        for element in data:\n",
    "            row = {}\n",
    "            row['class_1'] = element[0]\n",
    "            row['class_2'] = element[1]\n",
    "            row['chunks_source'] = element[2]\n",
    "            row['chunks_translation'] = element[3]\n",
    "\n",
    "            sentence = element[4]\n",
    "            row['text_source'] = sentence[0]\n",
    "            row['text_translation'] = sentence[1]\n",
    "            new_df = new_df.append(row, ignore_index=True)\n",
    "    return new_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, chunk_source, chunk_translation, targets):\n",
    "        self.chunk_source = chunk_source\n",
    "        self.chunk_translation = chunk_translation\n",
    "        self.pairs = [chunk_source, chunk_translation]\n",
    "        self.targets = targets\n",
    "        self.classes = list(np.unique(targets))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.pairs[0][idx], self.pairs[1][idx]), self.classes.index(self.targets[idx])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "class MyDatasetConnected(Dataset):\n",
    "    def __init__(self, chunk_source, chunk_translation, targets_1, targets_2):\n",
    "        self.chunk_source = chunk_source\n",
    "        self.chunk_translation = chunk_translation\n",
    "        self.pairs = [chunk_source, chunk_translation]\n",
    "        self.targets_1 = targets_1\n",
    "        self.targets_2 = targets_2\n",
    "        self.classes_1 = list(np.unique(targets_1))\n",
    "        self.classes_2 = list(np.unique(targets_2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets_1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.pairs[0][idx], self.pairs[1][idx]),\\\n",
    "               (self.classes_1.index(self.targets_1[idx]), self.classes_2.index(self.targets_2[idx]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "class ClassifierSeparate(nn.Module):\n",
    "    def __init__(self, df_labels, model_name='all-mpnet-base-v2'):\n",
    "        super().__init__()\n",
    "        self.classes = list(np.unique(df_labels))\n",
    "        self.sbert = SentenceTransformer(model_name)\n",
    "        self.train_sbert(True)\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=768, out_features=500),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=500, out_features=100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=100, out_features=len(self.classes)),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def train_sbert(self, train=True):\n",
    "        for param in self.sbert.parameters():\n",
    "            param.requires_grad = train\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        x = {\n",
    "            'input_ids': ids,\n",
    "            'attention_mask': mask\n",
    "        }\n",
    "        x = self.sbert(x)['sentence_embedding']\n",
    "        x = self.main(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "class ClassifierConnected(nn.Module):\n",
    "    def __init__(self, df_labels_1, df_labels_2, model_name='all-mpnet-base-v2'):\n",
    "        super().__init__()\n",
    "        self.classes_1 = list(np.unique(df_labels_1))\n",
    "        self.classes_2 = list(np.unique(df_labels_2))\n",
    "        self.sbert = SentenceTransformer(model_name)\n",
    "        self.train_sbert(False)\n",
    "        self.class_1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=768, out_features=len(self.classes_1)),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "        self.class_2 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=768, out_features=len(self.classes_2)),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def train_sbert(self, train=True):\n",
    "        for param in self.sbert.parameters():\n",
    "            param.requires_grad = train\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        x = {\n",
    "            'input_ids': ids,\n",
    "            'attention_mask': mask\n",
    "        }\n",
    "        x = self.sbert(x)['sentence_embedding']\n",
    "        return {\n",
    "            'class_1': self.class_1(x),\n",
    "            'class_2': self.class_2(x)\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "df = pd.read_xml(PATH_PROCESSED)\n",
    "df.drop('status', axis=1, inplace=True)\n",
    "\n",
    "df['source_split'] = df['source'].apply(source_to_words)\n",
    "df['translation_split'] = df['translation'].apply(source_to_words)\n",
    "\n",
    "split_columns = df['sentence'].str.split(pat='\\n', expand=True)\n",
    "df['source'] = split_columns[0]\n",
    "df['translation'] = split_columns[1]\n",
    "df.drop('sentence', axis=1, inplace=True)\n",
    "\n",
    "df['source'] = df['source'].apply(clear_sentence)\n",
    "df['translation'] = df['translation'].apply(clear_sentence)\n",
    "df['alignment'] = df['alignment'].apply(clear_alignment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "new_df = df_from_alignment(df['alignment'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "        class_1 class_2       chunks_source     chunks_translation  \\\n0          EQUI       5              (8, 9)               (11, 12)   \n1     SPE1_FACT       3  (1, 2, 3, 4, 5, 6)  (1, 2, 4, 5, 6, 7, 8)   \n2          EQUI       5                (7,)                  (10,)   \n3         NOALI       0                (0,)                   (3,)   \n4         NOALI       0                (0,)                   (9,)   \n...         ...     ...                 ...                    ...   \n3974      NOALI     NIL              (5, 6)                   (0,)   \n3975       SPE1       4                (2,)                   (2,)   \n3976       SIMI       2     (4, 5, 6, 7, 8)        (4, 5, 6, 7, 8)   \n3977       SPE2       3                (3,)                   (3,)   \n3978       EQUI       5                (1,)                   (1,)   \n\n                                 text_source  \\\n0                                      at 91   \n1     Former Nazi death camp guard Demjanjuk   \n2                                       dead   \n3                              -not aligned-   \n4                              -not aligned-   \n...                                      ...   \n3974                                 gear up   \n3975                                question   \n3976            in deadly LA boardwalk crash   \n3977                                     man   \n3978                                  Police   \n\n                                    text_translation  \n0                                            aged 91  \n1     John Demjanjuk convicted Nazi death camp guard  \n2                                               dies  \n3                                                  ,  \n4                                                  ,  \n...                                              ...  \n3974                                   -not aligned-  \n3975                                          arrest  \n3976                     in deadly LA driving attack  \n3977                                         suspect  \n3978                                          Police  \n\n[3979 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class_1</th>\n      <th>class_2</th>\n      <th>chunks_source</th>\n      <th>chunks_translation</th>\n      <th>text_source</th>\n      <th>text_translation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>EQUI</td>\n      <td>5</td>\n      <td>(8, 9)</td>\n      <td>(11, 12)</td>\n      <td>at 91</td>\n      <td>aged 91</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SPE1_FACT</td>\n      <td>3</td>\n      <td>(1, 2, 3, 4, 5, 6)</td>\n      <td>(1, 2, 4, 5, 6, 7, 8)</td>\n      <td>Former Nazi death camp guard Demjanjuk</td>\n      <td>John Demjanjuk convicted Nazi death camp guard</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>EQUI</td>\n      <td>5</td>\n      <td>(7,)</td>\n      <td>(10,)</td>\n      <td>dead</td>\n      <td>dies</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NOALI</td>\n      <td>0</td>\n      <td>(0,)</td>\n      <td>(3,)</td>\n      <td>-not aligned-</td>\n      <td>,</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NOALI</td>\n      <td>0</td>\n      <td>(0,)</td>\n      <td>(9,)</td>\n      <td>-not aligned-</td>\n      <td>,</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3974</th>\n      <td>NOALI</td>\n      <td>NIL</td>\n      <td>(5, 6)</td>\n      <td>(0,)</td>\n      <td>gear up</td>\n      <td>-not aligned-</td>\n    </tr>\n    <tr>\n      <th>3975</th>\n      <td>SPE1</td>\n      <td>4</td>\n      <td>(2,)</td>\n      <td>(2,)</td>\n      <td>question</td>\n      <td>arrest</td>\n    </tr>\n    <tr>\n      <th>3976</th>\n      <td>SIMI</td>\n      <td>2</td>\n      <td>(4, 5, 6, 7, 8)</td>\n      <td>(4, 5, 6, 7, 8)</td>\n      <td>in deadly LA boardwalk crash</td>\n      <td>in deadly LA driving attack</td>\n    </tr>\n    <tr>\n      <th>3977</th>\n      <td>SPE2</td>\n      <td>3</td>\n      <td>(3,)</td>\n      <td>(3,)</td>\n      <td>man</td>\n      <td>suspect</td>\n    </tr>\n    <tr>\n      <th>3978</th>\n      <td>EQUI</td>\n      <td>5</td>\n      <td>(1,)</td>\n      <td>(1,)</td>\n      <td>Police</td>\n      <td>Police</td>\n    </tr>\n  </tbody>\n</table>\n<p>3979 rows Ã— 6 columns</p>\n</div>"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "def change_classes(text):\n",
    "    class_1_change = {\n",
    "        'SPE1_FACT': 'SPE1',\n",
    "        'SIMI_FACT': 'SIMI',\n",
    "        'EQUI_POL': 'EQUI',\n",
    "        'EQUI_FACT': 'EQUI',\n",
    "        'REL_POL': 'REL',\n",
    "        'SPE2_FACT': 'SPE2',\n",
    "        'NOALI_FACT': 'NOALI',\n",
    "        'SPE2_POL': 'SPE2'\n",
    "    }\n",
    "    try:\n",
    "        new_class_name = class_1_change[text]\n",
    "        return new_class_name\n",
    "    except KeyError as e:\n",
    "        return text\n",
    "\n",
    "new_df['class_1'] = new_df['class_1'].apply(change_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "df_labels_1 = new_df.pop('class_1')\n",
    "df_labels_2 = new_df.pop('class_2')\n",
    "\n",
    "new_df.drop(columns=['chunks_source', 'chunks_translation'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "# Ktora klase chcemy uczyc? -> df_labels_1 albo df_labels_2\n",
    "df_labels = df_labels_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "df_train, df_test, y_train, y_test = train_test_split(new_df, df_labels, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(df_train['text_source'].values, df_train['text_translation'].values, y_train.values)\n",
    "test_dataset = MyDataset(df_test['text_source'].values, df_test['text_translation'].values, y_test.values)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "sbert = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "def tokenize(x):\n",
    "    return sbert.tokenize(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[   0, 1059,    2],\n         [   0, 1049,    2],\n         [   0, 1045,    2],\n         [   0, 1053,    2],\n         [   0, 1041,    2]]),\n 'attention_mask': tensor([[1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1],\n         [1, 1, 1]])}"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('siema')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, epochs, train_dataloader):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        for batch in train_dataloader:\n",
    "            data, labels = batch\n",
    "            data_batch = []\n",
    "            for s1, s2 in zip(data[0], data[1]):\n",
    "                data_batch.append((s1, s2))\n",
    "            data_batch = tokenize(data_batch)\n",
    "            labels = torch.tensor(labels, device=device)\n",
    "\n",
    "            labels = labels.to(device)\n",
    "            input_ids = data_batch['input_ids'].to(device)\n",
    "            attention_mask = data_batch['attention_mask'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            outputs = outputs.to(device)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "        print(f'EPOCH {[epoch]} | LOSS: {np.mean(losses)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "def get_accuracy(model, dataloader):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    metric= load_metric(\"accuracy\")\n",
    "    model.eval()\n",
    "    for batch in dataloader:\n",
    "        data, labels = batch\n",
    "        data_batch = []\n",
    "        for s1, s2 in zip(data[0], data[1]):\n",
    "            data_batch.append((s1, s2))\n",
    "        data_batch = tokenize(data_batch)\n",
    "\n",
    "        labels = torch.tensor(labels, device=device)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        labels = labels.to(device)\n",
    "        input_ids = data_batch['input_ids'].to(device)\n",
    "        attention_mask = data_batch['attention_mask'].to(device)\n",
    "\n",
    "        predictions = model(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(predictions, dim=1)\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "        metric.add_batch(predictions=predictions, references=labels)\n",
    "    return metric.compute()['accuracy'], y_true, y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "def get_classes_weight(df_labels):\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(df_labels), y=np.array(df_labels))\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "    return class_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "ClassifierSeparate(\n  (sbert): SentenceTransformer(\n    (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n    (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n    (2): Normalize()\n  )\n  (main): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=768, out_features=500, bias=True)\n    (2): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.2, inplace=False)\n    (4): ReLU()\n    (5): Linear(in_features=500, out_features=100, bias=True)\n    (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.2, inplace=False)\n    (8): ReLU()\n    (9): Linear(in_features=100, out_features=7, bias=True)\n    (10): Softmax(dim=None)\n  )\n)"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_separate = ClassifierSeparate(df_labels=df_labels)\n",
    "model_separate.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_separate = optim.Adam(model_separate.parameters(), lr=3e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH [0] | LOSS: 1.7906378022390395\n",
      "EPOCH [1] | LOSS: 1.7540829121766977\n",
      "EPOCH [2] | LOSS: 1.7207440113901493\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_7920/3098596194.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtrain_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel_separate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moptimizer_separate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m100\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_dataloader\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrain_dataloader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_7920/3217315577.py\u001B[0m in \u001B[0;36mtrain_model\u001B[1;34m(model, optimizer, epochs, train_dataloader)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m             \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m             \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_7920/288955712.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, ids, mask)\u001B[0m\n\u001B[0;32m     28\u001B[0m             \u001B[1;34m'attention_mask'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmask\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m         }\n\u001B[1;32m---> 30\u001B[1;33m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msbert\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'sentence_embedding'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     31\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     32\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    139\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    140\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 141\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    142\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, features)\u001B[0m\n\u001B[0;32m     64\u001B[0m             \u001B[0mtrans_features\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'token_type_ids'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfeatures\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'token_type_ids'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 66\u001B[1;33m         \u001B[0moutput_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mtrans_features\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreturn_dict\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     67\u001B[0m         \u001B[0moutput_tokens\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moutput_states\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001B[0m\n\u001B[0;32m    552\u001B[0m         \u001B[0mhead_mask\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_head_mask\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhead_mask\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_hidden_layers\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    553\u001B[0m         \u001B[0membedding_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0membeddings\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mposition_ids\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mposition_ids\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs_embeds\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs_embeds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 554\u001B[1;33m         encoder_outputs = self.encoder(\n\u001B[0m\u001B[0;32m    555\u001B[0m             \u001B[0membedding_output\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    556\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mextended_attention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001B[0m\n\u001B[0;32m    338\u001B[0m                 \u001B[0mall_hidden_states\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mall_hidden_states\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mhidden_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    339\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 340\u001B[1;33m             layer_outputs = layer_module(\n\u001B[0m\u001B[0;32m    341\u001B[0m                 \u001B[0mhidden_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    342\u001B[0m                 \u001B[0mattention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001B[0m\n\u001B[0;32m    297\u001B[0m         \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    298\u001B[0m     ):\n\u001B[1;32m--> 299\u001B[1;33m         self_attention_outputs = self.attention(\n\u001B[0m\u001B[0;32m    300\u001B[0m             \u001B[0mhidden_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    301\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001B[0m\n\u001B[0;32m    238\u001B[0m         \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    239\u001B[0m     ):\n\u001B[1;32m--> 240\u001B[1;33m         self_outputs = self.attn(\n\u001B[0m\u001B[0;32m    241\u001B[0m             \u001B[0mhidden_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    242\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001B[0m\n\u001B[0;32m    187\u001B[0m         \u001B[0mattention_probs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSoftmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mattention_scores\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    188\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 189\u001B[1;33m         \u001B[0mattention_probs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mattention_probs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    190\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    191\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mhead_mask\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\dropout.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 58\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtraining\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minplace\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     59\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mdropout\u001B[1;34m(input, p, training, inplace)\u001B[0m\n\u001B[0;32m   1167\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mp\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m0.0\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mp\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1.0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1168\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"dropout probability has to be between 0 and 1, \"\u001B[0m \u001B[1;34m\"but got {}\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1169\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_VF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdropout_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0minplace\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0m_VF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1170\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1171\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model=model_separate, optimizer=optimizer_separate, epochs=100, train_dataloader=train_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc, y_true, y_pred = get_accuracy(model_separate, test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=list(np.unique(df_labels))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train, df_test, y_train_1, y_test_1, y_train_2, y_test_2 = train_test_split(new_df, df_labels_1, df_labels_2, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = MyDatasetConnected(df_train['text_source'].values, df_train['text_translation'].values, y_train_1.values, y_train_2.values)\n",
    "test_dataset = MyDatasetConnected(df_test['text_source'].values, df_test['text_translation'].values, y_test_1.values, y_test_2.values)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_connected = ClassifierConnected(df_labels_1=df_labels_1, df_labels_2=df_labels_2)\n",
    "model_connected.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_connected = optim.Adam(model_connected.parameters(), lr=3e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_connected.train()\n",
    "for epoch in range(100):\n",
    "    losses = []\n",
    "    for batch in train_dataloader:\n",
    "        data, labels = batch\n",
    "        data_batch = []\n",
    "        for s1, s2 in zip(data[0], data[1]):\n",
    "            data_batch.append((s1, s2))\n",
    "        data_batch = tokenize(data_batch)\n",
    "\n",
    "        label_1 = labels[0].to(device)\n",
    "        label_2 = labels[1].to(device)\n",
    "\n",
    "        input_ids = data_batch['input_ids'].to(device)\n",
    "        attention_mask = data_batch['attention_mask'].to(device)\n",
    "\n",
    "        optimizer_connected.zero_grad()\n",
    "        outputs = model_connected(input_ids, attention_mask)\n",
    "\n",
    "        output_1 = outputs['class_1']\n",
    "        output_2 = outputs['class_2']\n",
    "        output_1.to(device)\n",
    "        output_2.to(device)\n",
    "\n",
    "        loss_1 = criterion(output_1, label_1)\n",
    "        loss_2 = criterion(output_2, label_2)\n",
    "        loss = loss_1 + loss_2\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer_connected.step()\n",
    "    print(f'EPOCH {[epoch]} | LOSS: {np.mean(losses)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_connected.eval()\n",
    "for c in ['class_1', 'class_2']:\n",
    "    metric= load_metric(\"accuracy\")\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch in test_dataloader:\n",
    "        data, labels = batch\n",
    "        data_batch = []\n",
    "        for s1, s2 in zip(data[0], data[1]):\n",
    "            data_batch.append((s1, s2))\n",
    "        data_batch = tokenize(data_batch)\n",
    "\n",
    "        label_1 = labels[0].to(device)\n",
    "        label_2 = labels[1].to(device)\n",
    "\n",
    "        input_ids = data_batch['input_ids'].to(device)\n",
    "        attention_mask = data_batch['attention_mask'].to(device)\n",
    "\n",
    "        if c == 'class_1':\n",
    "            y_true.extend(label_1.cpu().numpy())\n",
    "            label_1.to(device)\n",
    "        else:\n",
    "            y_true.extend(label_2.cpu().numpy())\n",
    "            label_2.to(device)\n",
    "\n",
    "\n",
    "        predictions = model_connected(input_ids, attention_mask)\n",
    "        predictions = predictions[c]\n",
    "        predictions = torch.argmax(predictions, dim=1)\n",
    "        y_pred.extend(predictions.cpu().numpy())\n",
    "        if c == 'class_1':\n",
    "            metric.add_batch(predictions=predictions, references=label_1)\n",
    "        else:\n",
    "            metric.add_batch(predictions=predictions, references=label_2)\n",
    "    acc = accuracy = metric.compute()['accuracy']\n",
    "    print(f'CLASS {c} accuracy: {acc}')\n",
    "    print(classification_report(y_true, y_pred, target_names=list(np.unique(df_labels))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}